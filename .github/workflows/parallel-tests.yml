name: Parallel Test Execution

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:
    inputs:
      shard_count:
        description: 'Number of test shards'
        required: false
        default: '4'

permissions:
  contents: read
  issues: write
  pull-requests: write

env:
  PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
  GKE_REGION: europe-west2
  REGISTRY: europe-west2-docker.pkg.dev
  SHARD_COUNT: ${{ github.event.inputs.shard_count || '4' }}

jobs:
  # Job 1: Setup and build
  setup:
    name: Setup & Build
    runs-on: ubuntu-latest
    outputs:
      image-tag: ${{ steps.meta.outputs.image-tag }}
      has-node: ${{ steps.detect_node.outputs.has_node }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Detect Node dependencies
        id: detect_node
        run: |
          if find . -name "package-lock.json" -print -quit | grep -q .; then
            echo "has_node=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_node=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Setup Node.js
        if: ${{ steps.detect_node.outputs.has_node == 'true' }}
        uses: actions/setup-node@v6
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: 'frontend/package-lock.json'

      - name: Cache dependencies
        if: ${{ steps.detect_node.outputs.has_node == 'true' }}
        id: cache
        uses: actions/cache@v4
        with:
          path: |
            frontend/node_modules
            ~/.npm
          key: ${{ runner.os }}-node-${{ hashFiles('frontend/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-

      - name: Install dependencies
        if: steps.detect_node.outputs.has_node == 'true' && steps.cache.outputs.cache-hit != 'true'
        working-directory: frontend
        run: npm ci

      - name: Generate image metadata
        id: meta
        run: |
          IMAGE_TAG=$(echo "${{ github.sha }}" | cut -c1-7)
          echo "image-tag=${IMAGE_TAG}" >> "$GITHUB_OUTPUT"

      - name: Upload workspace
        uses: actions/upload-artifact@v5
        with:
          name: workspace
          path: |
            .
            !node_modules
          retention-days: 1

  # Job 2: Discover Terraform modules (auto-discovery)
  discover-terraform-modules:
    name: Discover Terraform Modules
    runs-on: ubuntu-latest
    needs: setup
    outputs:
      modules-with-tests: ${{ steps.find-modules.outputs.modules_with_tests }}
      module-count: ${{ steps.find-modules.outputs.count }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Find modules with tests
        id: find-modules
        run: |
          MODULES_WITH_TESTS=()
          for module_dir in terraform/modules/*/; do
            module_name=$(basename "$module_dir")
            if [ -d "${module_dir}tests" ] && [ -n "$(find "${module_dir}tests" -name '*.tftest.hcl' -print -quit)" ]; then
              MODULES_WITH_TESTS+=("$module_name")
            fi
          done

          MODULES_JSON=$(printf '%s\n' "${MODULES_WITH_TESTS[@]}" | jq -R -s -c 'split("\n")[:-1]')
          echo "modules_with_tests=$MODULES_JSON" >> "$GITHUB_OUTPUT"
          echo "count=${#MODULES_WITH_TESTS[@]}" >> "$GITHUB_OUTPUT"
          echo "üîç Found ${#MODULES_WITH_TESTS[@]} modules with tests"

  # Job 3: Consolidated Terraform validation (all modules in single job)
  terraform-validate-all:
    name: Validate All Terraform Modules
    runs-on: ubuntu-latest
    needs: [setup, discover-terraform-modules]
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.11.0

      - name: Cache Terraform plugins
        uses: actions/cache@v4
        with:
          path: ~/.terraform.d/plugin-cache
          key: ${{ runner.os }}-terraform-${{ hashFiles('terraform/modules/**/.terraform.lock.hcl') }}
          restore-keys: |
            ${{ runner.os }}-terraform-

      - name: Validate all modules
        run: |
          echo "üîç Validating all Terraform modules..."
          FAILED=0
          PASSED=0

          for module_dir in terraform/modules/*/; do
            module_name=$(basename "$module_dir")
            echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
            echo "üì¶ $module_name"
            echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"

            cd "$module_dir"

            if terraform init -backend=false > /dev/null 2>&1 && \
               terraform validate > /dev/null 2>&1 && \
               terraform fmt -check -recursive > /dev/null 2>&1; then
              echo "  ‚úÖ Pass"
              PASSED=$((PASSED + 1))
            else
              echo "  ‚ùå Fail"
              FAILED=$((FAILED + 1))
              terraform init -backend=false
              terraform validate
            fi

            cd "$GITHUB_WORKSPACE"
          done

          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          echo "üìä Summary: ‚úÖ $PASSED passed, ‚ùå $FAILED failed"
          [ $FAILED -eq 0 ] || exit 1

  # Job 4: Terraform tests (parallel by module for speed)
  terraform-test:
    name: Test Module (${{ matrix.module }})
    runs-on: ubuntu-latest
    needs: [discover-terraform-modules, terraform-validate-all]
    if: ${{ needs.discover-terraform-modules.outputs.modules-with-tests != '[]' }}
    strategy:
      fail-fast: false
      matrix:
        module: ${{ fromJson(needs.discover-terraform-modules.outputs.modules-with-tests) }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.11.0

      - name: Terraform Init
        working-directory: terraform/modules/${{ matrix.module }}
        run: terraform init -backend=false

      - name: Terraform Test
        working-directory: terraform/modules/${{ matrix.module }}
        run: terraform test

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: terraform-test-results-${{ matrix.module }}
          path: terraform/modules/${{ matrix.module }}/.terraform/tests/
          retention-days: 7
          if-no-files-found: ignore

  # Job 4: Unit tests (parallel shards)
  unit-tests:
    name: Unit Tests (Shard ${{ matrix.shardIndex }}/${{ matrix.shardTotal }})
    runs-on: ubuntu-latest
    needs: setup
    if: ${{ needs.setup.outputs.has-node == 'true' }}
    strategy:
      fail-fast: false
      matrix:
        shardIndex: [1, 2, 3, 4]
        shardTotal: [4]

    steps:
      - name: Download workspace
        uses: actions/download-artifact@v6
        with:
          name: workspace

      - name: Setup Node.js
        uses: actions/setup-node@v6
        with:
          node-version: '20'

      - name: Restore dependencies cache
        uses: actions/cache@v4
        with:
          path: |
            frontend/node_modules
            ~/.npm
          key: ${{ runner.os }}-node-${{ hashFiles('frontend/package-lock.json') }}

      - name: Check if test script exists
        id: check_test
        working-directory: frontend
        run: |
          if npm run | grep -q "^  test$"; then
            echo "has_test=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_test=false" >> "$GITHUB_OUTPUT"
            echo "‚ö†Ô∏è test script not found in package.json, skipping unit tests"
          fi

      - name: Run unit tests (Shard ${{ matrix.shardIndex }}/${{ matrix.shardTotal }})
        if: steps.check_test.outputs.has_test == 'true'
        working-directory: frontend
        run: |
          npm test -- \
            --shard=${{ matrix.shardIndex }}/${{ matrix.shardTotal }} \
            --coverage \
            --coverageDirectory=coverage-${{ matrix.shardIndex }} \
            --maxWorkers=4
        env:
          NODE_ENV: test
          SHARD_INDEX: ${{ matrix.shardIndex }}
          SHARD_TOTAL: ${{ matrix.shardTotal }}

      - name: Upload coverage
        uses: actions/upload-artifact@v5
        with:
          name: coverage-unit-${{ matrix.shardIndex }}
          path: frontend/coverage-${{ matrix.shardIndex }}/
          retention-days: 7

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: test-results-unit-${{ matrix.shardIndex }}
          path: frontend/test-results/
          retention-days: 7

  # Job 5: Integration tests (parallel by service)
  integration-tests:
    name: Integration Tests (${{ matrix.service }})
    runs-on: ubuntu-latest
    needs: setup
    if: ${{ needs.setup.outputs.has-node == 'true' }}
    strategy:
      fail-fast: false
      matrix:
        service:
          - conversation-manager
          - llm-gateway
          - knowledge-base
          - ticket-monitor
          - action-executor
          - notification-service
          - api-gateway
          - analytics-service

    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: testpassword
          POSTGRES_DB: testdb-${{ matrix.service }}
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: Download workspace
        uses: actions/download-artifact@v6
        with:
          name: workspace

      - name: Setup Node.js
        uses: actions/setup-node@v6
        with:
          node-version: '20'

      - name: Restore dependencies cache
        uses: actions/cache@v4
        with:
          path: |
            frontend/node_modules
            ~/.npm
          key: ${{ runner.os }}-node-${{ hashFiles('frontend/package-lock.json') }}

      - name: Check if test:integration script exists
        id: check_test
        working-directory: frontend
        run: |
          if npm run | grep -q "^  test:integration$"; then
            echo "has_test=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_test=false" >> "$GITHUB_OUTPUT"
            echo "‚ö†Ô∏è test:integration script not found in package.json, skipping integration tests"
          fi

      - name: Run integration tests for ${{ matrix.service }}
        if: steps.check_test.outputs.has_test == 'true'
        working-directory: frontend
        run: |
          npm run test:integration -- \
            --testPathPattern=${{ matrix.service }} \
            --coverage \
            --coverageDirectory=coverage-integration-${{ matrix.service }} \
            --maxWorkers=2
        env:
          NODE_ENV: test
          SERVICE_NAME: ${{ matrix.service }}
          DATABASE_URL: postgresql://postgres:testpassword@localhost:5432/testdb-${{ matrix.service }}
          REDIS_URL: redis://localhost:6379

      - name: Upload coverage
        uses: actions/upload-artifact@v5
        with:
          name: coverage-integration-${{ matrix.service }}
          path: frontend/coverage-integration-${{ matrix.service }}/
          retention-days: 7

  # Job 6: E2E tests with isolated Kubernetes namespaces
  e2e-tests:
    name: E2E Tests (Shard ${{ matrix.shardIndex }}/${{ matrix.shardTotal }})
    runs-on: ubuntu-latest
    needs: [setup, unit-tests]
    if: ${{ needs.setup.outputs.has-node == 'true' }}
    strategy:
      fail-fast: false
      matrix:
        shardIndex: [1, 2, 3]
        shardTotal: [3]

    steps:
      - name: Download workspace
        uses: actions/download-artifact@v6
        with:
          name: workspace

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'v1.28.0'

      - name: Setup kind (Kubernetes in Docker)
        id: kind
        uses: helm/kind-action@v1.13.0
        with:
          cluster_name: test-cluster-${{ matrix.shardIndex }}

      - name: Create isolated namespace
        if: steps.kind.outcome == 'success'
        run: |
          kubectl create namespace e2e-shard-${{ matrix.shardIndex }}
          kubectl label namespace e2e-shard-${{ matrix.shardIndex }} \
            test-shard=${{ matrix.shardIndex }} \
            test-isolation=true

      - name: Deploy test infrastructure
        if: steps.kind.outcome == 'success'
        run: |
          # Check if k8s test manifests exist
          if [ -d "k8s/test" ]; then
            # Apply test-specific manifests
            if [ -f "k8s/test/postgres.yaml" ]; then
              kubectl apply -f k8s/test/postgres.yaml -n e2e-shard-${{ matrix.shardIndex }}
            fi
            if [ -f "k8s/test/redis.yaml" ]; then
              kubectl apply -f k8s/test/redis.yaml -n e2e-shard-${{ matrix.shardIndex }}
            fi

            # Wait for infrastructure if postgres was deployed
            if [ -f "k8s/test/postgres.yaml" ]; then
              kubectl wait --for=condition=ready pod \
                -l app=postgres \
                -n e2e-shard-${{ matrix.shardIndex }} \
                --timeout=300s || echo "‚ö†Ô∏è Infrastructure pods not ready"
            fi
          else
            echo "‚ö†Ô∏è k8s/test directory not found, skipping infrastructure deployment"
          fi

      - name: Deploy microservices
        if: steps.kind.outcome == 'success'
        run: |
          # Check if k8s deployments directory exists
          if [ -d "k8s/deployments" ]; then
            DEPLOYED=false
            # Deploy services with test configuration
            for service in conversation-manager llm-gateway knowledge-base; do
              if [ -f "k8s/deployments/${service}.yaml" ]; then
                kubectl apply -f k8s/deployments/${service}.yaml \
                  -n e2e-shard-${{ matrix.shardIndex }}
                DEPLOYED=true
              else
                echo "‚ö†Ô∏è k8s/deployments/${service}.yaml not found, skipping"
              fi
            done

            # Wait for pods to be ready if any were deployed
            if [ "$DEPLOYED" = true ]; then
              kubectl wait --for=condition=ready pod \
                -l tier=backend \
                -n e2e-shard-${{ matrix.shardIndex }} \
                --timeout=300s || echo "‚ö†Ô∏è Service pods not ready"
            fi
          else
            echo "‚ö†Ô∏è k8s/deployments directory not found, skipping microservices deployment"
          fi

      - name: Check if test:e2e script exists
        id: check_test
        working-directory: frontend
        run: |
          if npm run | grep -q "^  test:e2e$"; then
            echo "has_test=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_test=false" >> "$GITHUB_OUTPUT"
            echo "‚ö†Ô∏è test:e2e script not found in package.json, skipping E2E tests"
          fi

      - name: Run E2E tests (Shard ${{ matrix.shardIndex }}/${{ matrix.shardTotal }})
        if: steps.check_test.outputs.has_test == 'true' && steps.kind.outcome == 'success'
        working-directory: frontend
        run: |
          npm run test:e2e -- \
            --shard=${{ matrix.shardIndex }}/${{ matrix.shardTotal }} \
            --maxWorkers=2
        env:
          NODE_ENV: test
          NAMESPACE: e2e-shard-${{ matrix.shardIndex }}
          KUBECONFIG: ${{ env.KUBECONFIG }}

      - name: Collect pod logs on failure
        if: failure() && steps.kind.outcome == 'success'
        run: |
          mkdir -p logs
          kubectl get pods -n "e2e-shard-${{ matrix.shardIndex }}" -o name | while IFS= read -r pod; do
            kubectl logs "${pod}" -n "e2e-shard-${{ matrix.shardIndex }}" \
              > "logs/$(basename "${pod}").log" 2>&1 || true
          done

      - name: Upload pod logs
        if: failure()
        uses: actions/upload-artifact@v5
        with:
          name: pod-logs-shard-${{ matrix.shardIndex }}
          path: logs/
          retention-days: 7

      - name: Cleanup namespace
        if: always() && steps.kind.outcome == 'success'
        run: |
          kubectl delete namespace e2e-shard-${{ matrix.shardIndex }} --wait=false

  # Job 7: Security tests (parallel by category)
  security-tests:
    name: Security Tests (${{ matrix.category }})
    runs-on: ubuntu-latest
    needs: setup
    if: ${{ needs.setup.outputs.has-node == 'true' }}
    strategy:
      fail-fast: false
      matrix:
        category:
          - authentication
          - authorization
          - data-protection
          - network-security
          - secrets-management

    steps:
      - name: Download workspace
        uses: actions/download-artifact@v6
        with:
          name: workspace

      - name: Setup Node.js
        uses: actions/setup-node@v6
        with:
          node-version: '20'

      - name: Restore dependencies cache
        uses: actions/cache@v4
        with:
          path: |
            frontend/node_modules
            ~/.npm
          key: ${{ runner.os }}-node-${{ hashFiles('frontend/package-lock.json') }}

      - name: Check if test:security script exists
        id: check_test
        working-directory: frontend
        run: |
          if npm run | grep -q "^  test:security$"; then
            echo "has_test=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_test=false" >> "$GITHUB_OUTPUT"
            echo "‚ö†Ô∏è test:security script not found in package.json, skipping security tests"
          fi

      - name: Run security tests for ${{ matrix.category }}
        if: steps.check_test.outputs.has_test == 'true'
        working-directory: frontend
        run: |
          npm run test:security -- \
            --testPathPattern=${{ matrix.category }} \
            --maxWorkers=2
        env:
          SECURITY_TEST_CATEGORY: ${{ matrix.category }}

  # Job 8: Performance tests (parallel by endpoint)
  performance-tests:
    name: Performance Tests (${{ matrix.endpoint }})
    runs-on: ubuntu-latest
    needs: [setup, integration-tests]
    strategy:
      fail-fast: false
      matrix:
        endpoint:
          - api-gateway
          - llm-gateway
          - knowledge-base
          - conversation-manager

    steps:
      - name: Download workspace
        uses: actions/download-artifact@v6
        with:
          name: workspace

      - name: Setup k6 (load testing)
        run: |
          curl https://github.com/grafana/k6/releases/download/v0.47.0/k6-v0.47.0-linux-amd64.tar.gz -L | tar xvz
          sudo mv k6-v0.47.0-linux-amd64/k6 /usr/local/bin/

      - name: Check if performance test exists
        id: check_test
        run: |
          if [ -f "tests/performance/${{ matrix.endpoint }}.k6.js" ]; then
            echo "has_test=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_test=false" >> "$GITHUB_OUTPUT"
            echo "‚ö†Ô∏è tests/performance/${{ matrix.endpoint }}.k6.js not found, skipping performance test"
          fi

      - name: Run performance tests for ${{ matrix.endpoint }}
        if: steps.check_test.outputs.has_test == 'true'
        run: |
          k6 run \
            --out json=results-${{ matrix.endpoint }}.json \
            --summary-export=summary-${{ matrix.endpoint }}.json \
            tests/performance/${{ matrix.endpoint }}.k6.js
        env:
          K6_VUS: 50
          K6_DURATION: 2m
          TARGET_ENDPOINT: ${{ matrix.endpoint }}

      - name: Upload performance results
        uses: actions/upload-artifact@v5
        with:
          name: performance-results-${{ matrix.endpoint }}
          path: |
            results-${{ matrix.endpoint }}.json
            summary-${{ matrix.endpoint }}.json
          retention-days: 30

  # Job 9: Aggregate results and generate report
  aggregate-results:
    name: Aggregate Test Results
    runs-on: ubuntu-latest
    needs:
      [
        setup,
        terraform-test,
        unit-tests,
        integration-tests,
        e2e-tests,
        security-tests,
        performance-tests,
      ]
    if: ${{ always() && needs.setup.outputs.has-node == 'true' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Download all artifacts
        uses: actions/download-artifact@v6
        with:
          path: artifacts/

      - name: Setup Node.js
        uses: actions/setup-node@v6
        with:
          node-version: '20'

      - name: Merge coverage reports
        run: |
          npm install -g nyc

          # Create coverage directories
          mkdir -p coverage/unit coverage/integration coverage/combined

          # Merge unit test coverage if artifacts exist
          if ls artifacts/coverage-unit-* 1> /dev/null 2>&1; then
            echo "Merging unit test coverage..."
            nyc merge artifacts/coverage-unit-* coverage/unit
          else
            echo "No unit test coverage artifacts found, skipping..."
          fi

          # Merge integration test coverage if artifacts exist
          if ls artifacts/coverage-integration-* 1> /dev/null 2>&1; then
            echo "Merging integration test coverage..."
            nyc merge artifacts/coverage-integration-* coverage/integration
          else
            echo "No integration test coverage artifacts found, skipping..."
          fi

          # Generate combined report if any coverage exists
          if [ -d "coverage/unit" ] || [ -d "coverage/integration" ]; then
            echo "Generating combined coverage report..."
            nyc report \
              --reporter=lcov \
              --reporter=text \
              --reporter=html \
              --report-dir=coverage/combined || echo "Coverage report generation skipped (no coverage data)"
          else
            echo "No coverage data available, skipping report generation..."
          fi

      - name: Upload combined coverage
        if: hashFiles('coverage/combined/lcov.info') != ''
        uses: codecov/codecov-action@v5
        with:
          directory: coverage/combined
          flags: combined
          fail_ci_if_error: false

      - name: Generate test summary
        run: |
          cat > test-summary.md << 'EOF'
          # Test Execution Summary

          ## Parallel Execution Statistics

          | Test Type | Shards | Status |
          |-----------|--------|--------|
          | Terraform | 12 modules | ${{ needs.terraform-test.result }} |
          | Unit Tests | 4 shards | ${{ needs.unit-tests.result }} |
          | Integration | 8 services | ${{ needs.integration-tests.result }} |
          | E2E Tests | 3 shards | ${{ needs.e2e-tests.result }} |
          | Security | 5 categories | ${{ needs.security-tests.result }} |
          | Performance | 4 endpoints | ${{ needs.performance-tests.result }} |
          EOF

          # Add coverage summary if available
          if [ -f "coverage/combined/lcov.info" ]; then
            LINES_COVERED=$(grep -c -E '^(SF|DA)' coverage/combined/lcov.info)
            {
              echo ""
              echo "## Coverage Summary"
              echo ""
              echo "$LINES_COVERED lines covered"
            } >> test-summary.md
          fi

          # Add performance benchmarks if available
          if ls artifacts/performance-results-*/summary-*.json 1> /dev/null 2>&1; then
            {
              echo ""
              echo "## Performance Benchmarks"
              echo ""
            } >> test-summary.md

            if ls artifacts/performance-results-api-gateway/summary-*.json 1> /dev/null 2>&1; then
              P95=$(cat artifacts/performance-results-api-gateway/summary-*.json | jq -r '.metrics.http_req_duration.values.p95' 2>/dev/null || echo "N/A")
              echo "- API Gateway: ${P95}ms (p95)" >> test-summary.md
            fi

            if ls artifacts/performance-results-llm-gateway/summary-*.json 1> /dev/null 2>&1; then
              P95=$(cat artifacts/performance-results-llm-gateway/summary-*.json | jq -r '.metrics.http_req_duration.values.p95' 2>/dev/null || echo "N/A")
              echo "- LLM Gateway: ${P95}ms (p95)" >> test-summary.md
            fi
          fi

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('test-summary.md', 'utf8');

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });

      - name: Upload test summary
        uses: actions/upload-artifact@v5
        with:
          name: test-summary
          path: |
            test-summary.md
            coverage/combined/
          retention-days: 30

  # Job 10: Notify results
  notify:
    name: Notify Results
    runs-on: ubuntu-latest
    needs: aggregate-results
    if: always()
    env:
      SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}

    steps:
      - name: Notify Slack on success
        if: success() && env.SLACK_WEBHOOK != ''
        run: |
          COMMIT_SHA="${{ github.sha }}"
          curl -X POST "${{ env.SLACK_WEBHOOK }}" \
            -H 'Content-Type: application/json' \
            -d "{
              \"text\": \"‚úÖ Parallel test suite passed\",
              \"attachments\": [{
                \"color\": \"good\",
                \"title\": \"Test Execution Complete\",
                \"fields\": [
                  {\"title\": \"Total Shards\", \"value\": \"32\", \"short\": true},
                  {\"title\": \"Duration\", \"value\": \"~8 minutes\", \"short\": true},
                  {\"title\": \"Commit\", \"value\": \"${COMMIT_SHA}\", \"short\": false}
                ]
              }]
            }"

      - name: Notify Slack on failure
        if: failure() && env.SLACK_WEBHOOK != ''
        run: |
          COMMIT_SHA="${{ github.sha }}"
          curl -X POST "${{ env.SLACK_WEBHOOK }}" \
            -H 'Content-Type: application/json' \
            -d "{
              \"text\": \"‚ùå Parallel test suite failed\",
              \"attachments\": [{
                \"color\": \"danger\",
                \"title\": \"Test Execution Failed\",
                \"text\": \"Check GitHub Actions for details\",
                \"fields\": [
                  {\"title\": \"Commit\", \"value\": \"${COMMIT_SHA}\", \"short\": false}
                ]
              }]
            }"
