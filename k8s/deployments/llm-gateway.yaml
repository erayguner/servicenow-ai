---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-gateway
  namespace: production
  labels:
    app: llm-gateway
    tier: backend
    version: v1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: llm-gateway
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app: llm-gateway
        tier: backend
        version: v1
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '8080'
    spec:
      serviceAccountName: llm-gateway-sa
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
        seccompProfile:
          type: RuntimeDefault
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values: [llm-gateway]
                topologyKey: kubernetes.io/hostname

      # Node Affinity for AI workload nodes
      nodeSelector:
        workload: ai-inference
      tolerations:
        - key: workload
          operator: Equal
          value: ai-inference
          effect: NoSchedule
      containers:
        - name: llm-gateway
          image: europe-west2-docker.pkg.dev/PROJECT_ID/ai-services/llm-gateway:latest
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 8080
            - name: metrics
              containerPort: 9090
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
            capabilities:
              drop: [ALL]
          resources:
            requests:
              memory: 1Gi
              cpu: 1000m
            limits:
              memory: 4Gi
              cpu: 2000m
          env:
            - name: PORT
              value: '8080'
            - name: NODE_ENV
              value: production
            - name: VERTEX_AI_PROJECT
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: VERTEX_AI_LOCATION
              value: europe-west4
            - name: MODEL_TIMEOUT
              value: '60000'
            - name: MAX_TOKENS
              value: '4096'
          volumeMounts:
            - name: tmp
              mountPath: /tmp
            - name: model-cache
              mountPath: /app/model-cache
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8080
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 15
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3
      volumes:
        - name: tmp
          emptyDir: {}
        - name: model-cache
          emptyDir:
            sizeLimit: 5Gi
---
apiVersion: v1
kind: Service
metadata:
  name: llm-gateway
  namespace: production
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 80
      targetPort: 8080
  selector:
    app: llm-gateway
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-gateway
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-gateway
  minReplicas: 3
  maxReplicas: 15
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 75
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 85
