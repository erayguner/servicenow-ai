---
# NVIDIA GPU Operator for GPU resource management
# This manifest installs the GPU Operator which automates GPU node configuration

# Namespace for GPU Operator
apiVersion: v1
kind: Namespace
metadata:
  name: gpu-operator
---
# GPU Operator Helm values ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: gpu-operator-values
  namespace: gpu-operator
data:
  values.yaml: |
    # NVIDIA GPU Operator Configuration
    operator:
      defaultRuntime: containerd
      cleanupCRD: false

    # Node Feature Discovery (NFD) - auto-detect GPU nodes
    nfd:
      enabled: true

    # GPU Feature Discovery (GFD) - label nodes with GPU capabilities
    gfd:
      enabled: true

    # Device Plugin - expose GPUs to Kubernetes
    devicePlugin:
      enabled: true
      config:
        name: time-slicing-config-all
        default: any

    # DCGM Exporter - GPU metrics for Prometheus
    dcgmExporter:
      enabled: true
      serviceMonitor:
        enabled: true
        interval: 15s

    # GPU Driver - auto-install NVIDIA drivers (if needed)
    driver:
      enabled: false  # Set to true if nodes don't have pre-installed drivers

    # Toolkit - NVIDIA Container Toolkit
    toolkit:
      enabled: true
      version: v1.14.3

    # MIG Manager - Multi-Instance GPU support
    migManager:
      enabled: false  # Enable for A100/H100 with MIG support

    # Node Status Exporter - node-level metrics
    nodeStatusExporter:
      enabled: true
---
# ConfigMap for GPU time-slicing (share GPUs across pods)
apiVersion: v1
kind: ConfigMap
metadata:
  name: time-slicing-config-all
  namespace: gpu-operator
data:
  any: |-
    version: v1
    flags:
      migStrategy: none
    sharing:
      timeSlicing:
        renameByDefault: false
        failRequestsGreaterThanOne: false
        resources:
          - name: nvidia.com/gpu
            replicas: 4  # Allow 4 pods to share each GPU
---
# PriorityClass for GPU workloads
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: gpu-workload-high
value: 1000000
globalDefault: false
description: High priority for production LLM workloads
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: gpu-workload-medium
value: 500000
globalDefault: false
description: Medium priority for development LLM workloads
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: gpu-workload-low
value: 100000
globalDefault: false
description: Low priority for batch/experimental workloads
---
# RuntimeClass for GPU workloads with NVIDIA container runtime
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
---
# ResourceQuota for GPU namespace (prevent GPU exhaustion)
apiVersion: v1
kind: ResourceQuota
metadata:
  name: gpu-quota
  namespace: production
spec:
  hard:
    requests.nvidia.com/gpu: 20 # Max 20 GPUs in production namespace
    limits.nvidia.com/gpu: 20
---
# LimitRange for GPU pods (ensure reasonable resource requests)
apiVersion: v1
kind: LimitRange
metadata:
  name: gpu-limits
  namespace: production
spec:
  limits:
    - type: Container
      max:
        nvidia.com/gpu: 4 # Max 4 GPUs per container
      min:
        nvidia.com/gpu: 0
      default:
        nvidia.com/gpu: 1
      defaultRequest:
        nvidia.com/gpu: 1
---
# ServiceMonitor for GPU metrics (Prometheus)
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: dcgm-exporter
  namespace: gpu-operator
  labels:
    app: dcgm-exporter
spec:
  selector:
    matchLabels:
      app: nvidia-dcgm-exporter
  endpoints:
    - port: metrics
      interval: 15s
      path: /metrics
---
# Example pod using GPU with time-slicing
apiVersion: v1
kind: Pod
metadata:
  name: gpu-test
  namespace: production
spec:
  restartPolicy: OnFailure
  priorityClassName: gpu-workload-medium
  runtimeClassName: nvidia
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 2000
    seccompProfile:
      type: RuntimeDefault
  containers:
    - name: cuda-test
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        readOnlyRootFilesystem: true
        allowPrivilegeEscalation: false
        capabilities:
          drop:
            - ALL
      image: nvidia/cuda:12.1.0-base-ubuntu22.04
      command: [nvidia-smi]
      volumeMounts:
        - name: tmp
          mountPath: /tmp
      resources:
        requests:
          cpu: 100m
          memory: 256Mi
          nvidia.com/gpu: 1
        limits:
          cpu: 500m
          memory: 512Mi
          nvidia.com/gpu: 1
  volumes:
    - name: tmp
      emptyDir: {}
  nodeSelector:
    cloud.google.com/gke-accelerator: nvidia-tesla-t4
  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule
---
# DaemonSet for GPU health checks
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: gpu-health-check
  namespace: gpu-operator
spec:
  selector:
    matchLabels:
      app: gpu-health-check
  template:
    metadata:
      labels:
        app: gpu-health-check
    spec:
      hostPID: true
      hostIPC: true
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      nodeSelector:
        cloud.google.com/gke-accelerator-count: "1" # Only on GPU nodes
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: health-check
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
          image: nvidia/cuda:12.1.0-base-ubuntu22.04
          command: [/bin/bash, -c]
          args:
            - |
              while true; do
                echo "Running GPU health check..."
                nvidia-smi --query-gpu=timestamp,name,temperature.gpu,utilization.gpu,utilization.memory,memory.total,memory.free,memory.used --format=csv
                sleep 60
              done
          volumeMounts:
            - name: tmp
              mountPath: /tmp
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
              nvidia.com/gpu: 1
            limits:
              cpu: 500m
              memory: 512Mi
              nvidia.com/gpu: 1
      volumes:
        - name: tmp
          emptyDir: {}
---
# ConfigMap for GPU monitoring alerts
apiVersion: v1
kind: ConfigMap
metadata:
  name: gpu-alerts
  namespace: gpu-operator
data:
  alerts.yaml: |-
    groups:
      - name: gpu-alerts
        interval: 30s
        rules:
          - alert: GPUHighTemperature
            expr: DCGM_FI_DEV_GPU_TEMP > 85
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "GPU temperature is high"
              description: "GPU {{ $labels.gpu }} on {{ $labels.node }} has temperature {{ $value }}Â°C"
          - alert: GPUMemoryHigh
            expr: (DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_FREE) > 0.9
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "GPU memory usage is high"
              description: "GPU {{ $labels.gpu }} on {{ $labels.node }} has {{ $value }}% memory used"
          - alert: GPUXidError
            expr: rate(DCGM_FI_DEV_XID_ERRORS[5m]) > 0
            labels:
              severity: critical
            annotations:
              summary: "GPU Xid errors detected"
              description: "GPU {{ $labels.gpu }} on {{ $labels.node }} reported Xid errors"
          - alert: GPUPowerThrottling
            expr: DCGM_FI_DEV_POWER_VIOLATION > 0
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "GPU power throttling detected"
              description: "GPU {{ $labels.gpu }} on {{ $labels.node }} is being power throttled"
