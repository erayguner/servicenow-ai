# üöÄ Foundational Models - Quick Start

## üéØ Recommended: Hybrid Routing (Best of Both Worlds!)

**Automatically routes between self-hosted (fast/cheap) and cloud
(long-context/complex)**

### Deploy Hybrid Router (2 commands)

```bash
# 1. Deploy self-hosted + foundational models
kubectl apply -f k8s/llm-serving/kserve-runtime.yaml
kubectl apply -f k8s/llm-serving/foundational-models.yaml

# 2. Deploy hybrid router (intelligent routing)
kubectl apply -f k8s/llm-serving/hybrid-routing.yaml
```

---

## Alternative: Foundational Models Only

### 1Ô∏è‚É£ Deploy (1 command)

```bash
kubectl apply -f k8s/llm-serving/foundational-models.yaml
```

## 2Ô∏è‚É£ Configure Credentials

### Google Vertex AI (No keys needed!)

‚úÖ Already configured via Workload Identity

### OpenAI

```bash
kubectl create secret generic openai-api-key \
  --from-literal=api-key=sk-YOUR_KEY \
  -n production
```

### Anthropic

```bash
kubectl create secret generic anthropic-api-key \
  --from-literal=api-key=sk-ant-YOUR_KEY \
  -n production
```

## 3Ô∏è‚É£ Use It!

### Hybrid Routing (Recommended)

**Automatic intelligent routing** - just use `model: "auto"`:

```python
import requests

url = 'http://hybrid-llm-router.production/v1/chat/completions'

# Simple query ‚Üí Self-hosted (fast & cheap)
response = requests.post(url, json={
    'model': 'auto',  # Automatic routing
    'messages': [{'role': 'user', 'content': 'What is Kubernetes?'}]
})
# Uses: self-hosted/mistral-7b-instruct

# Long document ‚Üí Cloud (1M context)
response = requests.post(url, json={
    'model': 'auto',
    'messages': [{'role': 'user', 'content': f'{long_doc}\n\nSummarize this'}]
})
# Uses: vertex-ai/gemini-1.5-pro (1M tokens)

print(response.json()['choices'][0]['message']['content'])
```

### Explicit Strategy Selection

```python
# Fast response (self-hosted, <500ms)
response = requests.post(url, json={
    'model': 'auto',
    'routing_strategy': 'fast',
    'messages': [{'role': 'user', 'content': 'Quick summary'}]
})

# Maximum quality (premium cloud)
response = requests.post(url, json={
    'model': 'auto',
    'routing_strategy': 'quality',
    'messages': [{'role': 'user', 'content': 'Complex analysis'}]
})

# Minimum cost (prefer self-hosted)
response = requests.post(url, json={
    'model': 'auto',
    'routing_strategy': 'cost',
    'messages': [{'role': 'user', 'content': 'Translate this'}]
})
```

### cURL Example

```bash
# Auto routing
curl -X POST http://hybrid-llm-router.production/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "auto",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'

# Fast routing
curl -X POST http://hybrid-llm-router.production/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "auto",
    "routing_strategy": "fast",
    "messages": [{"role": "user", "content": "Quick response needed"}]
  }'
```

## üìä Routing Strategies (Hybrid)

| Strategy       | Best For                 | Primary Models                      | Fallback      |
| -------------- | ------------------------ | ----------------------------------- | ------------- |
| `auto` ‚≠ê      | **Intelligent routing**  | Self-hosted ‚Üí Cloud (automatic)     | All available |
| `fast`         | Quick responses (<500ms) | Self-hosted Mistral (disaggregated) | Cloud fast    |
| `quality`      | Complex tasks            | Claude Opus, GPT-4, Gemini Pro      | Cloud quality |
| `cost`         | Budget-conscious         | Self-hosted Mistral ($0.01/1M)      | Gemini Flash  |
| `long_context` | Large documents (>100K)  | Gemini Pro (1M), Claude (200K)      | Cloud long    |
| `balanced`     | General purpose          | Mix of self-hosted + cloud          | Balanced      |

**Cost Savings with Hybrid**: ~70% vs cloud-only **Speed Improvement**: ~50%
faster for simple queries

## üí∞ Pricing (per 1M tokens)

| Provider        | Model         | Input      | Output    | Context |
| --------------- | ------------- | ---------- | --------- | ------- |
| **Self-Hosted** | Mistral 7B    | **$0.01**  | **$0.02** | 32K     |
| **Self-Hosted** | CodeLlama 13B | **$0.015** | **$0.03** | 100K    |
| Vertex AI       | Gemini Flash  | $0.10      | $0.30     | 1M      |
| Vertex AI       | Gemini Pro    | $3.50      | $10.50    | 1M      |
| OpenAI          | GPT-3.5       | $0.50      | $1.50     | 16K     |
| OpenAI          | GPT-4         | $10.00     | $30.00    | 128K    |
| Anthropic       | Claude Haiku  | $0.25      | $1.25     | 200K    |
| Anthropic       | Claude Opus   | $15.00     | $75.00    | 200K    |

**Hybrid Routing Tip**: Use `model='auto'` for automatic cost optimization!

- Simple queries ‚Üí Self-hosted ($0.01/1M) = **97% cheaper than cloud**
- Long context ‚Üí Gemini Pro (1M tokens)
- Complex reasoning ‚Üí Premium cloud models

## üéØ Common Use Cases

### ServiceNow Ticket Summarization

```python
response = requests.post(url, json={
    'routing_strategy': 'fast',
    'messages': [{'role': 'user', 'content': ticket_text}],
    'max_tokens': 100
})
```

### Knowledge Base Search (Long Context)

```python
response = requests.post(url, json={
    'routing_strategy': 'long_context',
    'messages': [{'role': 'user', 'content': f'{context}\n\n{question}'}]
})
```

### Code Generation

```python
response = requests.post(url, json={
    'routing_strategy': 'code',
    'messages': [{'role': 'user', 'content': 'Write Python REST API'}]
})
```

## üîç Testing

```bash
# Test router
./scripts/test-llm-deployment.sh

# Check logs
kubectl logs -n production deployment/llm-router

# View metrics
kubectl port-forward -n production svc/llm-router 9090:9090
# Open http://localhost:9090/metrics
```

## üìö Full Documentation

- **Complete Guide**: `docs/FOUNDATIONAL_MODELS_GUIDE.md`
- **LLM Deployment**: `docs/LLM_DEPLOYMENT_GUIDE.md`
- **Configuration**: `k8s/llm-serving/foundational-models.yaml`

---

**Status**: ‚úÖ Production Ready **Support**: ai-infrastructure@company.com
